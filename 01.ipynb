{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c50f8a6-0493-4c8a-b20d-cf77e43520e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6be9760-b446-451b-bda6-e44cd9c7acd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0+cu128\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "294ae633-a0d4-4abd-bbad-f74b903f85f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1,2,3],[4,5,6]])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9682760a-3df1-4ac7-b6f3-c5cc2948cf95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b407a87-119b-458d-86e5-30d10b0d5ebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a933c5e-de0a-4678-9eb8-4c8567dc2b42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d871271f-bab1-4180-825f-af2d1e166a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.empty((2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c11d5d53-ed77-4c85-adb8-f7f96bbecba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.6512e+04,  1.5596e-42],\n",
       "        [ 0.0000e+00,  0.0000e+00]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a3d0413e-a6fb-48dd-afd1-e888552a038c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros((2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b06863d2-f406-4a16-9c5b-485f5430986e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "707325fa-e4ca-4658-bf3e-5c896eb13748",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros((2,2), dtype = int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "45025814-33ab-4fa3-9f7f-1583a61c7980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0],\n",
       "        [0, 0]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dc7acde2-4304-4e31-92bd-d23e03dcac81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6626, 0.3670, 0.2235, 0.2612])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6f556986-090c-4260-90df-ad3b1b9c9732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9295, 0.2251, 0.5631, 0.3447],\n",
       "        [0.8454, 0.1034, 0.8417, 0.7178],\n",
       "        [0.0831, 0.9507, 0.7409, 0.1183],\n",
       "        [0.4065, 0.4737, 0.7591, 0.3138]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6e8f9aac-4bf0-495b-a333-a26c8fe043c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "\"check_uniform_bounds\" not implemented for 'Long'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m torch\u001b[38;5;241m.\u001b[39mrand((\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m4\u001b[39m), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: \"check_uniform_bounds\" not implemented for 'Long'"
     ]
    }
   ],
   "source": [
    "torch.rand((4,4), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b85fd0b7-2873-4c4f-9dc6-703ae9d19c13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0.],\n",
       "        [0., 1.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eye(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c6f7b188-004d-4e8a-869a-69f31a835493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0],\n",
       "        [0, 1]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eye(2,2, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "22de02ee-396d-47d1-84cf-a096da829d2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1fb42491-23bf-4ed2-891b-b14acfbcec3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(1,5,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "608ea68e-ace6-4a85-b96d-fd796c9ba2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.diag(torch.tensor([1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "94a9722f-d376-499c-b0cf-e5dccca01b14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0],\n",
       "        [0, 2, 0],\n",
       "        [0, 0, 3]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "95ab1f5f-4729-4344-a179-ef3d3b1fdc36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False, False],\n",
       "        [False,  True, False],\n",
       "        [False, False,  True]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "99fe37a9-ec03-445c-89aa-547bfecfc238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0],\n",
       "        [0, 2, 0],\n",
       "        [0, 0, 3]], dtype=torch.int16)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.short()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "653c73e1-4cf7-4c3c-a177-32ba1c4e31a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 2., 0.],\n",
       "        [0., 0., 3.]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fc370884-34d6-41b7-b50f-51e9f20d0b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "06fb4fc9-b176-42da-99a7-fc6bba9252e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.zeros((3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cfd4980a-3e38-41d3-992a-98fa8ee81947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ee7fddd8-ba01-48e0-b130-d2bb3210a61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "t =torch.from_numpy(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4d788e83-96c9-4372-83f0-9af13339115f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "53e743b5-7fb0-4ae9-bf5d-659663ab87a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = t.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1ebdf9fc-932b-43a8-bfe0-ae94ead0d69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x =  torch.tensor([1,2,3])\n",
    "y = torch.tensor([4,5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "125abb98-77a3-4698-969a-6e46b4cecb0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 7, 9])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.add(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c421f371-d8de-4f58-a506-3a86b5a7c833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 7, 9])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x+y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9d4cda78-47cd-4b7b-b233-588094b2f7f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 3, 3])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y-x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6e061e14-dbf4-4c7b-bcf5-eb7557fa19ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.0000, 2.5000, 2.0000])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y/x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "52f87524-0484-4d06-bea1-5c3697d2b137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 7, 9])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.add(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d76a79e9-2894-43f2-8840-60fae2ece6c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b295a7cf-1f23-495a-bf3f-7d9c25735232",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9, 12, 15])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.add_(y)#inplce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f313413c-0bf4-4304-8f90-0d06903aae3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9, 12, 15])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "bb5088d0-b1b2-40cc-827b-aecb5b12f1c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0164cb0b-0302-4ec7-b0b7-ce009e3103cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.rand((2,5))\n",
    "x2 = torch.rand((5,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0c4af492-49b7-458c-8d3f-65dd5b46b196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0591, 0.6618, 1.2926],\n",
       "        [1.3858, 0.8887, 1.4134]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(x1,x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e6aac4e7-ef14-4562-bca6-1adfba4ea6ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0591, 0.6618, 1.2926],\n",
       "        [1.3858, 0.8887, 1.4134]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.mm(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "cf7d42da-fa1f-4353-98be-4469f4f406a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0591, 0.6618, 1.2926],\n",
       "        [1.3858, 0.8887, 1.4134]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1@x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "1574e28b-f2e7-452d-93ab-01e1f0c1d060",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.rand((5,5))\n",
    "x2 = torch.rand((1,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "0a9346c7-7333-4cbe-959a-1f6e17bd676a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0342, 0.1267, 0.5581, 0.2236, 0.0235],\n",
       "         [0.0093, 0.0094, 0.7368, 0.2549, 0.0613],\n",
       "         [0.4196, 0.4238, 0.5091, 0.2255, 0.2806],\n",
       "         [0.0529, 0.2920, 0.4533, 0.4771, 0.2579],\n",
       "         [0.5884, 0.0661, 0.3211, 0.3416, 0.1504]]),\n",
       " tensor([[0.2516, 0.5918, 0.8701, 0.6481, 0.9264]]))"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1,x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a1ac7a23-5827-4f5a-9576-f3a6b814b217",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2175, -0.4651, -0.3120, -0.4245, -0.9030],\n",
       "        [-0.2423, -0.5824, -0.1333, -0.3932, -0.8651],\n",
       "        [ 0.1680, -0.1680, -0.3610, -0.4226, -0.6458],\n",
       "        [-0.1987, -0.2998, -0.4167, -0.1710, -0.6685],\n",
       "        [ 0.3368, -0.5257, -0.5489, -0.3065, -0.7760]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1-x2 #broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f3161cae-2c90-428c-b521-276f05dedc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = torch.rand((2,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "33fa177a-fe4e-404c-8c92-6329584d0d28",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (5) must match the size of tensor b (2) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[149], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m x1\u001b[38;5;241m-\u001b[39mx2\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (5) must match the size of tensor b (2) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "x1-x2 #broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "de7485a6-533f-4a9a-95dd-506868bb664a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "bf05b5af-ce2b-4dcf-883c-8e07c93c2e92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9, 12, 15])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "f0f3c847-c9c5-466c-a369-f614d700cd96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7368)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "0dcea77e-2e53-4265-b158-5bedd742bb5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([0.5884, 0.4238, 0.7368, 0.4771, 0.2806]),\n",
       "indices=tensor([4, 2, 1, 3, 2]))"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(x1, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "9a6fc69d-f8a8-4a2a-88bd-734b5b09aae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10, 12, 15])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.clamp(x ,min=10) # clamps every value which is less than 10 to 10 > kinda Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdbb810-05f5-4691-ae23-8d07b0f5c2d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "989f18ac-2594-46de-ae1a-75d0ac50aa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =10\n",
    "features = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "98f147af-1c06-4c1b-9eef-37464a5f9035",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((batch_size, features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "4c158205-0688-48e3-ad6c-39609c237ec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 25])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "fbb99813-741d-462f-9dbe-18f6651385ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3467, 0.6655, 0.1635, 0.5400, 0.0358, 0.4740, 0.3850, 0.6049, 0.9957,\n",
       "        0.5833, 0.6898, 0.4065, 0.8471, 0.3490, 0.1911, 0.3142, 0.0011, 0.3067,\n",
       "        0.4068, 0.6222, 0.7289, 0.9200, 0.4148, 0.8884, 0.7108])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "06f8f738-4f75-45e1-99fc-0d2872fa4f40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3467, 0.5493, 0.9208, 0.2840, 0.3391, 0.2580, 0.4026, 0.6963, 0.1739,\n",
       "        0.1247])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "fd9baf7a-cfd6-40dd-a615-1e9f2596213c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9208, 0.6711, 0.2459, 0.9881, 0.0819, 0.0846, 0.8028, 0.4711, 0.1692,\n",
       "        0.1305])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[2,0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "7e64eafc-fa73-488f-8afb-736b170024c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3467)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "aab10e9f-4323-4966-8d59-87d40fc8c582",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0,0] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "b8c20f1a-b0d4-48fe-84d2-4645a7b92073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(100.)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "33d1d713-c889-42c5-befb-dde87b3a49a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+02, 6.6548e-01, 1.6346e-01, 5.4001e-01, 3.5805e-02, 4.7402e-01,\n",
       "        3.8496e-01, 6.0486e-01, 9.9575e-01, 5.8330e-01, 6.8983e-01, 4.0650e-01,\n",
       "        8.4707e-01, 3.4898e-01, 1.9109e-01, 3.1419e-01, 1.1196e-03, 3.0671e-01,\n",
       "        4.0677e-01, 6.2219e-01, 7.2891e-01, 9.2000e-01, 4.1483e-01, 8.8843e-01,\n",
       "        7.1076e-01])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "c643deb3-c9e8-4dea-8b62-b34139ec123e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "e5ba1a74-8edb-40d8-913e-8c60655237a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "b4b8f382-a35e-40ae-82db-13ca938d9481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[x>4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "aa8d176e-feed-4219-9008-9880b7a0d773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9, 10, 11, 12, 13, 14,  6,  7,  8,  9])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where(x>5 ,x, x+9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "9485b41b-3899-400c-89a4-9b7d7db6b49b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "d7c6fd31-3605-4fc5-ae9b-ad19bacaf08a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.ndimension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "35c47794-3091-41ea-a785-70c4dae6f449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.numel()#number of elemnets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "b86de471-4181-4cc1-811f-4faf9f9aa3d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "0ac12542-d0a0-418d-8ad7-a117824c4747",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[3, 3]' is invalid for input of size 10",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[210], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m3\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[3, 3]' is invalid for input of size 10"
     ]
    }
   ],
   "source": [
    "x.view(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "8318da23-4031-4e89-9ea3-5ef4f7aa6c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "27b842dd-57a0-4e48-876a-36011f425c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "d1216cc4-00e4-4dfe-8256-218f87dd0664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2365, 0.8970, 0.0642, 0.2845, 0.1895, 0.0299, 0.9382, 0.7476, 0.8534])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "a5d92e1e-7c48-473a-87f4-228db29132cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x.view(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "be27204d-94a9-4219-b69a-04aaf9a09612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5],\n",
       "        [6, 7, 8]])"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "1afaaa99-d35f-44a1-9297-b0d8986048a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5],\n",
       "        [6, 7, 8]])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.reshape(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "52791572-c1d3-429f-bc48-0e06bb657bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 3, 6],\n",
       "        [1, 4, 7],\n",
       "        [2, 5, 8]])"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "e785ab04-cf88-4607-aa9f-8c2178ee8c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.view(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "1f047b99-e530-42ce-9636-457c05d3b014",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[232], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y\u001b[38;5;241m.\u001b[39mt()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m9\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "source": [
    "y.t().view(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "b87e0a0d-8da6-4d4d-aa2c-be0c74b5efa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=torch.float64)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.contiguous().view(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "0bd333d4-0382-4f72-a0f2-003a1e076306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.view(-1)#flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "c449c1ef-5460-42a6-848d-0a8b7bd9f48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "227d0135-bbca-4891-8705-7895b47c36fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "ddff0d54-9d52-40fb-b59a-eecd8bbda93e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "aed0003f-0afd-4e0f-8d3e-1589a9c46c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "e198cbbe-d099-4a3e-9ea1-0d4ef4fd151b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [4],\n",
       "        [5],\n",
       "        [6],\n",
       "        [7],\n",
       "        [8],\n",
       "        [9]])"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "3359dfcb-1d0c-4fc0-9353-460db685a4b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[250], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m x\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "x.squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b87ea8-2649-4332-a9de-300f474c2bde",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "89517628-f5bb-445e-a2a3-45f6ed5b6014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "8a83bb90-1509-40db-82da-26ddcf872a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim # optimizers like adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "af098f2d-b695-4d6f-8869-65084f345409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F # Functions that dont have parameters like Relu, tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "ec699a27-8cc7-4e65-8f6a-d6f20bd00c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "8883f8e0-6611-4bad-912f-0d224d43a144",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create fully connected layer\n",
    "\n",
    "class NN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(NN, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size , 50) # x1w1*x2w2*….x784*w784 and then distribute to 50 neurons -> A\n",
    "        self.layer2 = nn.Linear(50 , num_classes)\n",
    "    def forward(self, x):#where connection between layers happens\n",
    "        x = F.relu(self.layer1(x))#for A we apply relu\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "d3295106-ad10-4631-bd1f-775d56df3233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "07641f89-5746-433e-8d8a-2639f6c41ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "c0cf6c0d-ecd2-4813-8f1f-49ea666f335c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "\n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 64 # divide input data into 64 > 60,000 ÷ 64 ≈ 937 batches\n",
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "3868a5f6-f4ed-4b31-9f02-212d69aedcd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 9.91M/9.91M [00:00<00:00, 20.6MB/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 28.9k/28.9k [00:00<00:00, 3.16MB/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1.65M/1.65M [00:00<00:00, 15.5MB/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 4.54k/4.54k [00:00<00:00, 3.76MB/s]\n"
     ]
    }
   ],
   "source": [
    "#data load\n",
    "\n",
    "train_data = datasets.MNIST(root = 'dataset/', train=True, transform=transforms.ToTensor(), download =True)\n",
    "test_data = datasets.MNIST(root = 'dataset/', train=False, transform=transforms.ToTensor(), download =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "9916e95b-3613-4fe4-92d7-8b4a7a81dee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset = train_data, batch_size = batch_size, shuffle =True)\n",
    "test_loader = DataLoader(dataset = test_data, batch_size = batch_size, shuffle =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "a12d9d05-fbb6-44f3-bfe4-7c4c5c4ecedd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1b353d1d580>"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "25c05a54-a127-4527-bcb9-37ff06369dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in test_loader:\n",
    "    print(images.shape)  # Shape of one batch\n",
    "    print(labels.shape)  # Shape of the labels\n",
    "    break  # Important! Otherwise it'll print all batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "276bd36b-f47c-4ce2-98c6-ac2491bf332d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize network\n",
    "model = NN(input_size = input_size, num_classes =num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "521d96a0-94e9-4e63-a573-9277dad1a555",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss and optimzer\n",
    "\n",
    "creiterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "a3735377-6230-4da6-a961-ff62b3c4d46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Train Network  ##############################\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    for batch_idx ,(data,targets) in enumerate(train_loader):\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device =device)\n",
    "\n",
    "        #changing the shape of input features in [1, 28, 28] into 784\n",
    "        data =  data.reshape(data.shape[0] ,-1) # [64, 1, 28, 28] == [64, -1] == [64, flatten=784]\n",
    "\n",
    "        #forward\n",
    "        scores = model(data)\n",
    "        loss = creiterion(scores,targets)\n",
    "\n",
    "        #backward\n",
    "        optimizer.zero_grad() # grad=0 for every run ..but weights will be using old iteration weights\n",
    "        loss.backward() # calculating gradient\n",
    "\n",
    "        optimizer.step()# updating the weight\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "f72dfb93-bc5f-480d-87e0-10e90fe58470",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader, model):\n",
    "    if loader.dataset.train:\n",
    "        print(\"Train datset check\")\n",
    "    else:\n",
    "        print(\"Test dataset check\")\n",
    "\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():#In with torch.no_grad(), you only use the model's already learned weights to predict outputs, without touching them.-scores = model(x)\n",
    "#          If you skip torch.no_grad(), then: PyTorch still tracks all operations for autograd.\n",
    "            # It builds a computation graph in memory even though you're not training.\n",
    "             # BUT (important!) it does NOT update weights automatically.\n",
    "        for x,y in loader:\n",
    "            x = x.to(device =device)\n",
    "            y = y.to(device =device)\n",
    "            x = x.reshape(x.shape[0],-1)\n",
    "\n",
    "            scores = model(x)\n",
    "            _,pred = scores.max(1)\n",
    "            num_correct += (pred == y).sum()\n",
    "            num_samples += pred.size(0)\n",
    "        print(f\" got {num_correct} / {num_samples} with accuracy of {float(num_correct/num_samples)*100:.2f}\")\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "46e39e0b-3732-4308-8416-2a78ffa36c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset check\n",
      " got 9276 / 10000 with accuracy of 92.76\n"
     ]
    }
   ],
   "source": [
    "check_accuracy(test_loader,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "7b55e0b1-ee92-435d-9f11-2b3fa6d6d06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train datset check\n",
      " got 55660 / 60000 with accuracy of 92.77\n"
     ]
    }
   ],
   "source": [
    "check_accuracy(train_loader,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "75f34fdf-b607-464a-b245-d831697c8505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9276, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics.classification import Accuracy\n",
    "\n",
    "metric = Accuracy(task=\"multiclass\", num_classes=10).to(device)\n",
    "\n",
    "for x, y in test_loader:\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    x = x.reshape(x.shape[0], -1)\n",
    "    scores = model(x)\n",
    "    _, pred = scores.max(1)\n",
    "    metric.update(pred, y)\n",
    "\n",
    "print(metric.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "34d3f71b-c139-43b9-a460-131f78474abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchmetricsNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading torchmetrics-1.7.1-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: numpy>1.20.0 in c:\\users\\jwali\\anaconda3\\lib\\site-packages (from torchmetrics) (1.26.4)\n",
      "Requirement already satisfied: packaging>17.1 in c:\\users\\jwali\\anaconda3\\lib\\site-packages (from torchmetrics) (24.1)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\jwali\\anaconda3\\lib\\site-packages (from torchmetrics) (2.7.0+cu128)\n",
      "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
      "  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jwali\\anaconda3\\lib\\site-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\jwali\\anaconda3\\lib\\site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.11.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\jwali\\anaconda3\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (3.13.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\jwali\\anaconda3\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\jwali\\anaconda3\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jwali\\anaconda3\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\jwali\\anaconda3\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (2024.6.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\jwali\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=2.0.0->torchmetrics) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jwali\\anaconda3\\lib\\site-packages (from jinja2->torch>=2.0.0->torchmetrics) (2.1.3)\n",
      "Downloading torchmetrics-1.7.1-py3-none-any.whl (961 kB)\n",
      "   ---------------------------------------- 0.0/961.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 961.5/961.5 kB 43.7 MB/s eta 0:00:00\n",
      "Downloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n",
      "Installing collected packages: lightning-utilities, torchmetrics\n",
      "Successfully installed lightning-utilities-0.14.3 torchmetrics-1.7.1\n"
     ]
    }
   ],
   "source": [
    "# pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934f1c69-6bd1-4b19-aa8f-2381a3a108a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0477bdf0-bbb0-4884-98ca-a1115a9ec959",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c447d7e7-5673-4238-ba1c-05b7649220d2",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "52d1616c-80d6-4b3b-bd45-709f4d269b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create fully connected layer\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_channels =1 , out_channels =10):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels , out_channels = 8, kernel_size = (3,3), stride =(1,1), padding=(1,1))\n",
    "        self.pool = nn.MaxPool2d(kernel_size = (2,2), stride=(2,2))\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels =16, kernel_size = (3,3), stride =(1,1), padding=(1,1))\n",
    "        self.layer1 = nn.Linear(16*7*7, num_classes)\n",
    "    def forward(self, x):#where connection between layers happens\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.reshape(x.shape[0], -1)#flatten\n",
    "        x = self.layer1(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "77ebcd6b-581d-4265-9c12-efb0c6640f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "\n",
    "#Hyperparameters\n",
    "\n",
    "in_channels = 1 #as it is black and white\n",
    "out_channels = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 64 # divide input data into 64 > 60,000 ÷ 64 ≈ 937 batches\n",
    "num_epochs = 1\n",
    "\n",
    "\n",
    "# #data load\n",
    "\n",
    "# train_data = datasets.MNIST(root = 'dataset/', train=True, transform=transforms.ToTensor(), download =True)\n",
    "# test_data = datasets.MNIST(root = 'dataset/', train=False, transform=transforms.ToTensor(), download =True)\n",
    "\n",
    "\n",
    "# train_loader = DataLoader(dataset = train_data, batch_size = batch_size, shuffle =True)\n",
    "# test_loader = DataLoader(dataset = test_data, batch_size = batch_size, shuffle =True)\n",
    "\n",
    "#initialize network\n",
    "model = CNN().to(device)\n",
    "\n",
    "#Loss and optimzer\n",
    "\n",
    "creiterion = nn.CrossEntropyLoss()#CrossEntropyLoss expects raw logits and internally applies softmax (normalization) to turn them into probabilities.\n",
    "optimizer = optim.Adam(model.parameters(),lr=learning_rate)\n",
    "\n",
    "########################### Train Network  ##############################\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    for batch_idx ,(data,targets) in enumerate(train_loader):\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device =device)\n",
    "\n",
    "        #changing the shape of input features in [1, 28, 28] into 784\n",
    "        # data =  data.reshape(data.shape[0] ,-1) # [64, 1, 28, 28] == [64, -1] == [64, flatten=784]\n",
    "\n",
    "        #forward\n",
    "        scores = model(data)\n",
    "        loss = creiterion(scores,targets)\n",
    "\n",
    "        #backward\n",
    "        optimizer.zero_grad() # grad=0 for every run ..but weights will be using old iteration weights\n",
    "        loss.backward() # calculating gradient\n",
    "\n",
    "        optimizer.step()# updating the weight\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "c428db90-83f5-4cbd-b256-b05ea5992bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset check\n",
      " got 9686 / 10000 with accuracy of 96.86\n"
     ]
    }
   ],
   "source": [
    "##############################Accuracy#####################\n",
    "def check_accuracy(loader, model):\n",
    "    if loader.dataset.train:\n",
    "        print(\"Train datset check\")\n",
    "    else:\n",
    "        print(\"Test dataset check\")\n",
    "\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():#In with torch.no_grad(), you only use the model's already learned weights to predict outputs, without touching them.-scores = model(x)\n",
    "#          If you skip torch.no_grad(), then: PyTorch still tracks all operations for autograd.\n",
    "            # It builds a computation graph in memory even though you're not training.\n",
    "             # BUT (important!) it does NOT update weights automatically.\n",
    "        for x,y in loader:\n",
    "            x = x.to(device =device)\n",
    "            y = y.to(device =device)\n",
    "            # x = x.reshape(x.shape[0],-1)\n",
    "\n",
    "            scores = model(x)\n",
    "            _,pred = scores.max(1)\n",
    "            num_correct += (pred == y).sum()\n",
    "            num_samples += pred.size(0)\n",
    "        print(f\" got {num_correct} / {num_samples} with accuracy of {float(num_correct/num_samples)*100:.2f}\")\n",
    "    model.train()\n",
    "        \n",
    "check_accuracy(test_loader,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd20a74-8b39-4a65-8837-27597deebcec",
   "metadata": {},
   "source": [
    "So first there will be inputs lets say in MNIST each photo has 784 pixels or inputs …so in first layer we will have 784 neurons and then we calculate x1w1*x2w2*….x784*w784 this value we pass to every hidden layer neuron say there are 50 hidden layers where lets say there will be activation RELU which makes value between -1 and +1 \n",
    "\n",
    "**Input layer has features, not neurons** → real neurons start in hidden layers. - Correction\n",
    "\n",
    "- Inputs = 784.\n",
    "- First hidden layer = 50 neurons (not 784 neurons again).\n",
    "\n",
    "\n",
    "\n",
    "so for all 50 values from hidden layer form this s shape distribution and again all will do forward calculation   x1w1*x2w2*….x50*w50 and lets say there is another hidden layer of 50 neurons again it does then dame finally there will be 10 neurons of output later\n",
    "\n",
    "this is total forward pass\n",
    "\n",
    "then we calculate  loss,\n",
    "\n",
    "then we calculate backward i.e gradient for all neurons\n",
    "\n",
    "then we do optimize\n",
    "\n",
    "lets say gradient is -0.02\n",
    "\n",
    "then w = -0.01* gradient (learning rate=0.01)\n",
    "\n",
    "we follow the same process for many times\n",
    "\n",
    "✅So **because of non-linear activations differentiation** and **gradient descent** can actually \"navigate\" complicated loss surfaces (full of hills, valleys, curves).\n",
    "\n",
    "✅ **you apply ReLU after each hidden layer**,\n",
    "\n",
    "✅ **except** after the **final output layer**.\n",
    "\n",
    "Each hidden layer needs **non-linearity** (ReLU) so the model can learn **complex transformations**.\n",
    "\n",
    "✅ **Bias is automatically included inside `nn.Linear` layer itself!**\n",
    "\n",
    "```python\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)  # 1st hidden layer\n",
    "        self.fc2 = nn.Linear(128, 64)           # 2nd hidden layer\n",
    "        self.fc3 = nn.Linear(64, 32)            # 3rd hidden layer\n",
    "        self.fc_out = nn.Linear(32, num_classes) # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc_out(x)  # No activation here\n",
    "        return x\n",
    "\n",
    "```\n",
    "\n",
    "**1 epoch meaning** -\n",
    "\n",
    "| Item | Value |\n",
    "| --- | --- |\n",
    "| Training data | 60,000 images |\n",
    "| Batch size | 64 |\n",
    "| Number of batches | 60,000 ÷ 64 ≈ 937 batches |\n",
    "\n",
    "➡️ In **one epoch**, your model **updates weights ~937 times**!\n",
    "\n",
    "**NOT** just once.\n",
    "\n",
    "Initial Weights (random)\n",
    "\n",
    "Batch 1:\n",
    "Inputs: Images 1-64\n",
    "Forward → Loss → Backward → Update weights\n",
    "\n",
    "Batch 2:\n",
    "Inputs: Images 65-128\n",
    "Forward (with new weights) → Loss → Backward → Update weights\n",
    "\n",
    "Batch 3:\n",
    "Inputs: Images 129-192\n",
    "Forward (with even newer weights) → Loss → Backward → Update weights\n",
    "\n",
    "...and so on, until all batches are done → 1 epoch completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8549da-b954-476d-8cea-ed9e15f15fbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
